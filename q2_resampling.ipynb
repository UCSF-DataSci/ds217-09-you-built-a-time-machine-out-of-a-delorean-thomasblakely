{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd9864b",
   "metadata": {},
   "source": [
    "# Question 2: Resampling and Frequency Conversion\n",
    "\n",
    "This question focuses on resampling operations and frequency conversion using ICU monitoring data (hourly) and patient vital signs data (daily).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5d645281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a454b02",
   "metadata": {},
   "source": [
    "## Part 2.1: Load and Prepare Data\n",
    "\n",
    "**Note:** These datasets have realistic characteristics:\n",
    "- **ICU Monitoring**: 75 patients with variable stay lengths (2-30 days). Not all patients are present for the entire 6-month period - patients are admitted and discharged at different times.\n",
    "- **Patient Vitals**: Already contains some missing visits (~5% missing data). This is realistic and will be useful for practicing missing data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b166d302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU monitoring shape: (86400, 7)\n",
      "Patient vitals shape: (18250, 7)\n",
      "\n",
      "ICU monitoring sample:\n",
      "                    patient_id  heart_rate  blood_pressure_systolic  \\\n",
      "datetime                                                              \n",
      "2023-01-01 00:00:00     ICU001   82.000000                      126   \n",
      "2023-01-01 01:00:00     ICU001   98.294095                      128   \n",
      "2023-01-01 02:00:00     ICU001  103.500000                      129   \n",
      "2023-01-01 03:00:00     ICU001   91.535534                      136   \n",
      "2023-01-01 04:00:00     ICU001   87.330127                      129   \n",
      "\n",
      "                     blood_pressure_diastolic  oxygen_saturation  temperature  \n",
      "datetime                                                                       \n",
      "2023-01-01 00:00:00                        65                 96    98.783988  \n",
      "2023-01-01 01:00:00                        67                 95    99.186212  \n",
      "2023-01-01 02:00:00                        68                 94    98.800638  \n",
      "2023-01-01 03:00:00                        75                 96    98.349004  \n",
      "2023-01-01 04:00:00                        68                 95    98.643958  \n",
      "\n",
      "Patient vitals sample:\n",
      "           patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-01-01      P0001    98.389672          71                      119   \n",
      "2023-01-02      P0001    98.492046          67                      117   \n",
      "2023-01-03      P0001    98.790163          70                      113   \n",
      "2023-01-04      P0001    98.635781          74                      117   \n",
      "2023-01-05      P0001    98.051660          67                      118   \n",
      "\n",
      "            blood_pressure_diastolic     weight  \n",
      "date                                             \n",
      "2023-01-01                        84  68.996865  \n",
      "2023-01-02                        82  67.720215  \n",
      "2023-01-03                        78  67.846825  \n",
      "2023-01-04                        82  67.693993  \n",
      "2023-01-05                        83  68.228852  \n",
      "\n",
      "ICU patients: 20\n",
      "ICU date range: 2023-01-01 00:00:00 to 2023-06-29 23:00:00\n",
      "\n",
      "Patient vitals patients: 50\n",
      "Patient vitals date range: 2023-01-01 00:00:00 to 2023-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# Load ICU monitoring data (hourly)\n",
    "icu_monitoring = pd.read_csv('data/icu_monitoring.csv')\n",
    "\n",
    "# Load patient vitals data (daily) - for comparison\n",
    "patient_vitals = pd.read_csv('data/patient_vitals.csv')\n",
    "\n",
    "print(\"ICU monitoring shape:\", icu_monitoring.shape)\n",
    "print(\"Patient vitals shape:\", patient_vitals.shape)\n",
    "\n",
    "# Convert datetime columns and set as index\n",
    "icu_monitoring['datetime'] = pd.to_datetime(icu_monitoring['datetime'])\n",
    "icu_monitoring = icu_monitoring.set_index('datetime')\n",
    "\n",
    "patient_vitals['date'] = pd.to_datetime(patient_vitals['date'])\n",
    "patient_vitals = patient_vitals.set_index('date')\n",
    "\n",
    "print(\"\\nICU monitoring sample:\")\n",
    "print(icu_monitoring.head())\n",
    "print(\"\\nPatient vitals sample:\")\n",
    "print(patient_vitals.head())\n",
    "\n",
    "# Check data characteristics\n",
    "print(f\"\\nICU patients: {icu_monitoring['patient_id'].nunique()}\")\n",
    "print(f\"ICU date range: {icu_monitoring.index.min()} to {icu_monitoring.index.max()}\")\n",
    "print(f\"\\nPatient vitals patients: {patient_vitals['patient_id'].nunique()}\")\n",
    "print(f\"Patient vitals date range: {patient_vitals.index.min()} to {patient_vitals.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7dd39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Part 2.2: Time Series Selection\n",
    "\n",
    "**‚ö†Ô∏è WARNING: Sort Index Before Date Selection!**\n",
    "Since multiple patients share the same date, the `patient_vitals` index is non-monotonic (not strictly increasing). **You MUST sort the index first** before using `.loc` with date ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f905616",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "patient_vitals = patient_vitals.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551db0cf",
   "metadata": {},
   "source": [
    "Without sorting, pandas cannot reliably handle date range selections and may return unexpected results or errors.\n",
    "\n",
    "**TODO: Perform time series indexing and selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f8e2fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 1, 2023 data:            patient_id  temperature  heart_rate  blood_pressure_systolic  \\\n",
      "date                                                                      \n",
      "2023-01-01      P0001    98.389672          71                      119   \n",
      "2023-01-01      P0024    97.552103          71                      111   \n",
      "2023-01-01      P0025    98.806201          83                      118   \n",
      "2023-01-01      P0047    98.943464          63                      110   \n",
      "2023-01-01      P0026    98.758551          75                      114   \n",
      "2023-01-01      P0027    98.462467          90                      119   \n",
      "2023-01-01      P0028    99.632166          71                      117   \n",
      "2023-01-01      P0029    98.881121          68                      121   \n",
      "2023-01-01      P0030    98.403614          73                      122   \n",
      "2023-01-01      P0031    98.410626          63                      113   \n",
      "2023-01-01      P0046    98.507553          89                      123   \n",
      "2023-01-01      P0032    99.547822          70                      123   \n",
      "2023-01-01      P0033    97.612817          56                      112   \n",
      "2023-01-01      P0034    99.075484          88                      124   \n",
      "2023-01-01      P0035    98.968285          62                      116   \n",
      "2023-01-01      P0036    99.589629          85                      119   \n",
      "2023-01-01      P0037    98.557134          64                      117   \n",
      "2023-01-01      P0038    98.137036          62                      129   \n",
      "2023-01-01      P0045    98.265412          67                      119   \n",
      "2023-01-01      P0039    98.746267          88                      128   \n",
      "2023-01-01      P0023    99.142508          65                      118   \n",
      "2023-01-01      P0022    97.700397          68                      122   \n",
      "2023-01-01      P0021    99.549514          81                      125   \n",
      "2023-01-01      P0020    98.572122          64                      113   \n",
      "2023-01-01      P0004    99.214044          73                      126   \n",
      "2023-01-01      P0005    98.075744          61                      119   \n",
      "2023-01-01      P0006    98.373337          69                      113   \n",
      "2023-01-01      P0007    99.005033          81                      131   \n",
      "2023-01-01      P0050    99.008145          74                      130   \n",
      "2023-01-01      P0008    97.987253          82                      111   \n",
      "2023-01-01      P0009    99.368380          90                      128   \n",
      "2023-01-01      P0010    99.244181          86                      123   \n",
      "2023-01-01      P0011    98.829135          81                      124   \n",
      "2023-01-01      P0040    99.024034          77                      123   \n",
      "2023-01-01      P0012    97.820562          64                      121   \n",
      "2023-01-01      P0049    98.124573          66                      120   \n",
      "2023-01-01      P0014    98.405814          76                      121   \n",
      "2023-01-01      P0015    99.039286          81                      112   \n",
      "2023-01-01      P0016    99.201435          66                      115   \n",
      "2023-01-01      P0002    98.432983          87                      114   \n",
      "2023-01-01      P0017    99.146657          87                      115   \n",
      "2023-01-01      P0018    97.867184          87                      125   \n",
      "2023-01-01      P0019    99.026862          81                      123   \n",
      "2023-01-01      P0048    98.519258          75                      118   \n",
      "2023-01-01      P0013    98.662933          69                      110   \n",
      "2023-01-01      P0041    98.452086          84                      131   \n",
      "2023-01-01      P0003    98.167264          83                      110   \n",
      "2023-01-01      P0043    98.958357          79                      113   \n",
      "2023-01-01      P0042    98.881607          78                      120   \n",
      "2023-01-01      P0044    98.345119          66                      114   \n",
      "\n",
      "            blood_pressure_diastolic      weight  \n",
      "date                                              \n",
      "2023-01-01                        84   68.996865  \n",
      "2023-01-01                        70   49.386068  \n",
      "2023-01-01                        65   84.096164  \n",
      "2023-01-01                        76   54.318578  \n",
      "2023-01-01                        75   49.830300  \n",
      "2023-01-01                        74   72.943755  \n",
      "2023-01-01                        86  104.977255  \n",
      "2023-01-01                        71   59.883550  \n",
      "2023-01-01                        79   60.754351  \n",
      "2023-01-01                        75   72.486551  \n",
      "2023-01-01                        66   62.109514  \n",
      "2023-01-01                        72   74.585881  \n",
      "2023-01-01                        73   51.137556  \n",
      "2023-01-01                        76   66.779965  \n",
      "2023-01-01                        79   76.864597  \n",
      "2023-01-01                        76   81.704275  \n",
      "2023-01-01                        84   78.116801  \n",
      "2023-01-01                        85   40.000000  \n",
      "2023-01-01                        73   76.812239  \n",
      "2023-01-01                        77   59.093381  \n",
      "2023-01-01                        83   49.643152  \n",
      "2023-01-01                        79   68.067771  \n",
      "2023-01-01                        73   77.967356  \n",
      "2023-01-01                        77   70.714935  \n",
      "2023-01-01                        82   93.836456  \n",
      "2023-01-01                        86   86.238145  \n",
      "2023-01-01                        81   52.923008  \n",
      "2023-01-01                        75   56.926071  \n",
      "2023-01-01                        81   75.383257  \n",
      "2023-01-01                        68   68.629172  \n",
      "2023-01-01                        70   62.387123  \n",
      "2023-01-01                        77   59.678637  \n",
      "2023-01-01                        80   60.073705  \n",
      "2023-01-01                        77   79.612071  \n",
      "2023-01-01                        71   48.410507  \n",
      "2023-01-01                        73   48.085497  \n",
      "2023-01-01                        75   82.570817  \n",
      "2023-01-01                        72   65.294840  \n",
      "2023-01-01                        82   82.512213  \n",
      "2023-01-01                        74   58.179729  \n",
      "2023-01-01                        84   81.641348  \n",
      "2023-01-01                        86   67.832031  \n",
      "2023-01-01                        73   70.635756  \n",
      "2023-01-01                        80   63.274809  \n",
      "2023-01-01                        81   57.631518  \n",
      "2023-01-01                        84   49.964682  \n",
      "2023-01-01                        70   79.302945  \n",
      "2023-01-01                        75   65.165670  \n",
      "2023-01-01                        72   58.710525  \n",
      "2023-01-01                        69   72.543359  \n",
      "Records on Jan 1: 50 (some patients may start later)\n",
      "January 2023 shape: (1550, 6)\n",
      "\n",
      "First quarter average temperature: 98.97¬∞F\n",
      "After June average temperature: 98.41¬∞F\n",
      "First week average temperature: 98.68¬∞F\n",
      "Last week average temperature: 98.65¬∞F\n",
      "Business hours data shape: (32400, 6)\n",
      "\n",
      "Average heart rate - All hours: 81.7 bpm\n",
      "Average heart rate - Business hours: 80.7 bpm\n",
      "Average temperature - All hours: 98.5¬∞F\n",
      "Average temperature - Business hours: 98.5¬∞F\n"
     ]
    }
   ],
   "source": [
    "# Select data by specific dates\n",
    "# Note: Not all patients may have data on January 1, 2023 (some start later)\n",
    "# Important: Sort the index first since multiple patients share the same date\n",
    "patient_vitals = patient_vitals.sort_index()  # Sort for reliable date-based selection\n",
    "january_first = patient_vitals.loc['2023-01-01']  # Select January 1, 2023 from patient_vitals\n",
    "print(\"January 1, 2023 data:\", january_first)\n",
    "print(f\"Records on Jan 1: {len(january_first)} (some patients may start later)\")\n",
    "\n",
    "#  Select data by date ranges\n",
    "january_data = patient_vitals.loc['2023-01']  # Select entire January 2023\n",
    "print(\"January 2023 shape:\", january_data.shape)\n",
    "\n",
    "# Select data by time periods\n",
    "first_quarter = patient_vitals.loc['2023Q1']  # Select Q1 2023\n",
    "entire_year = patient_vitals.loc['2023']  # Select all of 2023 (will include patients with partial year data)\n",
    "\n",
    "# Select first and last periods using .loc\n",
    "first_week = patient_vitals.loc[:patient_vitals.index.min() + pd.Timedelta(days=6)]  # First 7 days\n",
    "last_week = patient_vitals.loc[patient_vitals.index.max() - pd.Timedelta(days=6):]  # Last 7 days\n",
    "\n",
    "# Use truncate() method\n",
    "# Note: truncate() requires a sorted index. \n",
    "patient_vitals = patient_vitals.sort_index()\n",
    "data_after_june = patient_vitals.truncate(before = '2023-06-01')  # Truncate before June 1, 2023\n",
    "data_before_september = patient_vitals.truncate(after = '2023-08-31')  # Truncate after August 31, 2023\n",
    "\n",
    "# Use selected data for analysis\n",
    "# Compare average temperature between first quarter and data after June\n",
    "print(f\"\\nFirst quarter average temperature: {first_quarter['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"After June average temperature: {data_after_june['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"First week average temperature: {first_week['temperature'].mean():.2f}¬∞F\")\n",
    "print(f\"Last week average temperature: {last_week['temperature'].mean():.2f}¬∞F\")\n",
    "\n",
    "# For ICU data with time components:\n",
    "# Select business hours (9 AM to 5 PM)\n",
    "business_hours = icu_monitoring.between_time('09:00', '17:00')  # Use between_time()\n",
    "print(\"Business hours data shape:\", business_hours.shape)\n",
    "\n",
    "# Select specific time (noon readings)\n",
    "noon_data = icu_monitoring.at_time('12:00')  # Use at_time('12:00')\n",
    "\n",
    "# Use time-based selection for analysis\n",
    "# Compare vital signs during business hours vs other times\n",
    "all_hours_avg = icu_monitoring.select_dtypes(include=[np.number]).mean()\n",
    "business_hours_avg = business_hours.select_dtypes(include=[np.number]).mean()\n",
    "print(f\"\\nAverage heart rate - All hours: {all_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average heart rate - Business hours: {business_hours_avg['heart_rate']:.1f} bpm\")\n",
    "print(f\"Average temperature - All hours: {all_hours_avg['temperature']:.1f}¬∞F\")\n",
    "print(f\"Average temperature - Business hours: {business_hours_avg['temperature']:.1f}¬∞F\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f2fe",
   "metadata": {},
   "source": [
    "## Part 2.3: Resampling Operations\n",
    "\n",
    "**TODO: Perform resampling and frequency conversion**\n",
    "\n",
    "**Important Note:** When resampling DataFrames that contain non-numeric columns (like `patient_id`), you'll get an error if you try to aggregate them with numeric functions like `mean()`. Use `df.select_dtypes(include=[np.number])` to select only numeric columns before resampling, or specify which columns to aggregate in `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d18466a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICU daily shape: (180, 5)\n",
      "Weekly resampled shape: (53, 5)\n",
      "Monthly resampled shape: (12, 5)\n",
      "Missing values after upsampling: temperature                 323\n",
      "heart_rate                  323\n",
      "blood_pressure_systolic     323\n",
      "blood_pressure_diastolic    323\n",
      "weight                      323\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Resample hourly ICU data to daily\n",
    "# Note: Exclude non-numeric columns like 'patient_id' when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols = icu_monitoring.select_dtypes(include=[np.number]).columns\n",
    "icu_daily = icu_monitoring[numeric_cols].resample('D').mean()\n",
    "print(\"ICU daily shape:\", icu_daily.shape)\n",
    "\n",
    "# Resample daily patient data to weekly\n",
    "# Note: Exclude 'patient_id' column when resampling\n",
    "# Select only numeric columns before resampling\n",
    "numeric_cols_pv = patient_vitals.select_dtypes(include=[np.number]).columns\n",
    "patient_vitals_weekly = patient_vitals[numeric_cols_pv].resample('W').mean()\n",
    "print(\"Weekly resampled shape:\", patient_vitals_weekly.shape)\n",
    "\n",
    "# Resample daily patient data to monthly\n",
    "patient_vitals_monthly = patient_vitals[numeric_cols_pv].resample('ME').mean()  # Resample to monthly with mean aggregation (use freq='ME' for Month End)\n",
    "print(\"Monthly resampled shape:\", patient_vitals_monthly.shape)\n",
    "\n",
    "# Use different aggregation functions (mean, sum, max, min)\n",
    "icu_daily_stats = icu_monitoring[numeric_cols].resample('D').agg({\n",
    "    'heart_rate': ['mean', 'max', 'min'], \n",
    "    'temperature': 'mean',\n",
    "    'blood_pressure_diastolic' : ['mean', 'std']\n",
    "    })  # Resample with multiple aggregations\n",
    "\n",
    "\n",
    "# Handle missing values during resampling\n",
    "# Demonstrate upsampling (monthly to daily) creates missing values\n",
    "monthly_to_daily = patient_vitals_monthly.asfreq('D')  # Upsample monthly data to daily (use .asfreq())\n",
    "print(\"Missing values after upsampling:\", monthly_to_daily.isna().sum())\n",
    "\n",
    "# Compare different resampling frequencies\n",
    "# Create a DataFrame comparing resampling results at different frequencies\n",
    "# Important: Since patient_vitals contains multiple patients per date, aggregate by date first\n",
    "# to create a single daily time series for comparison.\n",
    "# Why aggregation is needed: The patient_vitals DataFrame has multiple rows per date (one for each patient),\n",
    "# so we need to average across patients for each date to create a single daily time series that can be\n",
    "# meaningfully compared with the weekly and monthly resampled data. Without aggregation, resampling would\n",
    "# operate on each patient's time series separately, making it difficult to compare frequencies meaningfully.\n",
    "# Steps:\n",
    "# 1. Since 'date' is currently the index, reset it to a column first, then aggregate by date\n",
    "#    Note: groupby('date').mean() automatically sets 'date' as the index in the result, so you don't need\n",
    "#    to call set_index('date') again after groupby.\n",
    "patient_vitals_reset = patient_vitals[numeric_cols_pv].reset_index()\n",
    "patient_vitals_daily_agg = patient_vitals_reset.groupby('date').mean()\n",
    "\n",
    "# The date is already the index after groupby, so no need to set_index again\n",
    "# 2. Compare the aggregated daily data with weekly and monthly resampled data\n",
    "# Use patient_vitals data resampled to different frequencies:\n",
    "# - Original daily data (aggregated by date): patient_vitals_daily_agg\n",
    "# - Weekly resampled (patient_vitals_weekly) \n",
    "# - Monthly resampled (patient_vitals_monthly)\n",
    "# Include columns: frequency, date_range, row_count, mean_temperature, std_temperature\n",
    "# Use the 'temperature' column from each resampled dataset\n",
    "resampling_comparison = pd.DataFrame({\n",
    "    'frequency': ['daily', 'weekly', 'monthly'],\n",
    "    'date_range': [\n",
    "        f\"{patient_vitals_daily_agg.index.min()}-{patient_vitals_daily_agg.index.max()}\",\n",
    "        f\"{patient_vitals_weekly.index.min()}-{patient_vitals_weekly.index.max()}\",\n",
    "        f\"{patient_vitals_monthly.index.min()}-{patient_vitals_monthly.index.max()}\"\n",
    "    ],  \n",
    "    'row_count': [\n",
    "        len(patient_vitals_daily_agg),\n",
    "        len(patient_vitals_weekly),\n",
    "        len(patient_vitals_monthly)\n",
    "        ],  \n",
    "    'mean_temperature': [\n",
    "        patient_vitals_daily_agg['temperature'].mean(),\n",
    "        patient_vitals_weekly['temperature'].mean(),\n",
    "        patient_vitals_monthly['temperature'].mean()\n",
    "    ], \n",
    "    'std_temperature': [\n",
    "        patient_vitals_daily_agg['temperature'].std(),\n",
    "        patient_vitals_weekly['temperature'].std(),\n",
    "        patient_vitals_monthly['temperature'].std()\n",
    "    ]   \n",
    "})\n",
    "\n",
    "# Save results as 'output/q2_resampling_analysis.csv'\n",
    "resampling_comparison.to_csv('output/q2_resampling_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3605486d",
   "metadata": {},
   "source": [
    "## Part 2.4: Missing Data Handling\n",
    "\n",
    "**üí° TIP: High Percentage of Missing Data is Expected!**\n",
    "When upsampling from monthly to daily frequency, you'll create approximately 96% missing data (only 12 month-end dates have values out of 365 days). This is normal and expected for upsampling - don't be alarmed!\n",
    "\n",
    "**Approach:** Create missing values by upsampling monthly data to daily frequency. This creates a clear, structured pattern of missing data that's ideal for practicing imputation methods.\n",
    "\n",
    "**TODO: Handle missing data in time series**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9dd87022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value count: 323\n",
      "Missing value percentage: 96.41791044776119\n"
     ]
    }
   ],
   "source": [
    "# Identify missing values in time series\n",
    "# Use the monthly resampled data from Part 2.3 and upsample to daily:\n",
    "#   - Take patient_vitals_monthly['temperature']\n",
    "#   - Upsample to daily frequency using .resample('D').asfreq()\n",
    "#   - This creates missing values for all days except month-end dates (~96% missing)\n",
    "ts_with_missing = patient_vitals_monthly['temperature'].resample('D').asfreq()  # Time series with missing values\n",
    "print(\"Missing value count:\", ts_with_missing.isna().sum())\n",
    "print(\"Missing value percentage:\", ts_with_missing.isna().sum() / len(ts_with_missing) * 100)\n",
    "\n",
    "# Use forward fill and backward fill\n",
    "ts_ffill = ts_with_missing.ffill()  # Forward fill missing values (use .ffill() method)\n",
    "ts_bfill = ts_with_missing.bfill()  # Backward fill missing values (use .bfill() method)\n",
    "\n",
    "# Use interpolation methods\n",
    "ts_interpolated = ts_with_missing.interpolate()  # Interpolate missing values\n",
    "ts_interpolated_linear = ts_with_missing.interpolate(method = 'linear')  # Linear interpolation\n",
    "ts_interpolated_time = ts_with_missing.interpolate(method = 'time')  # Time-based interpolation\n",
    "\n",
    "# Use rolling mean for imputation\n",
    "ts_rolling_imputed = ts_with_missing.fillna(ts_with_missing.rolling(window = 7).mean())  # Fill missing with rolling mean\n",
    "\n",
    "# Create missing data report\n",
    "# Document your missing data handling with the following sections:\n",
    "# 1. Missing value summary: Total count and percentage\n",
    "# 2. Missing data patterns: When/why data is missing (by month, day of week, etc.)\n",
    "# 3. Imputation method: Which method you used (forward fill, backward fill, interpolation, rolling mean)\n",
    "# 4. Rationale: Why you chose that method\n",
    "# 5. Pros and cons: Advantages and limitations of your approach\n",
    "# 6. Example: Show at least one example of missing data before and after imputation\n",
    "# Minimum length: 300 words\n",
    "missing_data_report = f\"\"\"\n",
    "Documentation of our missing data handling:\n",
    "\n",
    "- How many missing values did you find?\n",
    "    a. In our time series dataset, we found the number of missing values using: 'ts_with_missing.isna().sum()' \n",
    "    The number of missing values was: {ts_with_missing.isna().sum()}.\n",
    "\n",
    "- What percentage of data was missing?\n",
    "    a. We then calculated the percentage of data that was missing using: 'ts_with_missing.isna().sum() / len(ts_with_missing) * 100'\n",
    "    The percentage of data missing was: {ts_with_missing.isna().sum() / len(ts_with_missing) * 100}\n",
    "\n",
    "- Which method did you use to fill missing values?\n",
    "    a. In order to fill in the missing values, we tried the following methods: \n",
    "        (1) Forward Fill\n",
    "        (2) Backward Fill \n",
    "        (3) Interpolation (Linear)\n",
    "        (4) Interpolation (Time-based)\n",
    "        (5) Rolling Mean (7-Day)\n",
    "    Note that, because our dataset was so sparse originally, the 7-Day rolling mean did not actually fill in missing values.\n",
    "    Observe from the following code: 'ts_rolling_imputed.isna().sum()' ({ts_rolling_imputed.isna().sum()})\n",
    "\n",
    "- Why did you choose that method?\n",
    "    a. Ignoring the rolling mean method because of its inability to fill missing values in our data, I did not choose forward fill or backward fill. \n",
    "    While these methods would have done the job in filling in missing values, as roughly 96% of our data are missing values, we would essentially be generating a dataset with only {ts_ffill.nunique()} unique values.\n",
    "    As such, both methods fall flat for any prediction purposes. As for the linear and time-based interpolation methods, I ultimately would choose to go with the linear one.\n",
    "    From a technical standpoint, both methods should produce the same results given that there is no time irregularity in our dataset since the missingness was artificially introduced through upsampling.\n",
    "    \n",
    "- What are the pros/cons of your approach?\n",
    "    a. As for the advantage, linear interpolation is a relatively well-established method that provides a somewhat reasonable guess about our data's trends.\n",
    "    It doesn't use future information rashly, like with backward fill, which would otherwise cause issues if we tried to make statements about causality. \n",
    "    As for the cons, as we are using a linear interpolation method, we naturally have to assume that the behavior of temperature is linear. \n",
    "    In turn, we are naturally unable to capture the day-to-day fluctuations. Things like a patient getting sick and them taking antibiotics all gets smoothed over.\n",
    "\n",
    "- Include examples showing missing data patterns\n",
    "    a. Without imputation, we will see a temperature count on '2023-01-31', followed by 'NaN' until '2023-02-28'.\n",
    "    With imputation (linear), we will see that those 'NaN' gaps are filled.\n",
    "    {\n",
    "        pd.DataFrame({\n",
    "            'Original': ts_with_missing.head(29), \n",
    "            'Linear Interpolation': ts_interpolated_linear.head(29)\n",
    "        }).to_string()\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "# Document missing data patterns\n",
    "# Analyze when/why data is missing\n",
    "missing_by_month = ts_with_missing.groupby(ts_with_missing.index.month).apply(lambda x: x.isna().sum())\n",
    "missing_by_day = ts_with_missing.groupby(ts_with_missing.index.dayofweek).apply(lambda x: x.isna().sum())\n",
    "missing_patterns = f\"Missing by month:\\n{missing_by_month}\\n\\nMissing by day of week:\\n{missing_by_day}\"\n",
    "\n",
    "# Save results as 'output/q2_missing_data_report.txt'\n",
    "with open('output/q2_missing_data_report.txt', 'w') as f:\n",
    "    f.write(missing_data_report)\n",
    "    f.write(f\"\\n\\nMissing patterns:\\n{missing_patterns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cf350d",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before moving to Question 3, verify you've created:\n",
    "\n",
    "- [ ] `output/q2_resampling_analysis.csv` - resampling analysis results\n",
    "- [ ] `output/q2_missing_data_report.txt` - missing data handling report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
